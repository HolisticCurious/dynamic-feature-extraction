# -*- coding: utf-8 -*-
"""Dynamic Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dYVMR7t36qsn7n1d5WNkwdsGOpouLWsH

# ***Building DataFrames***

---
"""

#initial imports

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import matplotlib.cm as cm
import matplotlib as mpl
from scipy import ndimage 
import cv2
from PIL import Image
from __future__ import division, unicode_literals, print_function  # for compatibility with Python 2 and 3
from IPython.display import clear_output
from time import sleep
from RegionPropsMorph import RegionPropsMorph
from scipy.optimize import curve_fit
import pims
import trackpy as tp
from read_roi import read_roi_zip



#open excel with ROI info

roinfo = pd.read_csv('PATH_TO_FOLDER/Results.csv', index_col= 0) #bb stats from imagej


#open grayscale image to use for TrackPy
frames = pims.open('PATH_TO_FOLDER/IMAGE_NAME.tif')


#add columns to dataframe, to make measurements compatible w/ TrackPy
f = pd.DataFrame()


#IMPORTANT: UPDATE SCALE HERE (check imagej)
scalex = 3.7659 #pix/um
scaley = 3.75 #pix/um


#centroid coordinates
f['x'] = roinfo['X'].values*scalex
f['y'] = roinfo['Y'].values*scaley

#imagej saves with converted coordinates. So, here we are saving the conversion
f['x (um)'] = roinfo['X'].values
f['y (um)'] = roinfo['Y'].values
centroid = list(np.vstack((roinfo['X'].values,roinfo['Y'].values)).transpose())
f['centroid (um)'] = centroid

#saving timeframes
f['frame'] = roinfo['Slice'].values - 1

#saving bounding box coordinates, height and width
f['bx'] = roinfo['BX'].values*scalex
f['by'] = roinfo['BY'].values*scaley

f['w'] = roinfo['Width'].values*scalex
f['h'] = roinfo['Height'].values*scaley

#label name 
f['Label'] = roinfo['Label'].values


#visualize
plt.rcParams["figure.figsize"] = (10,10) #figure dimensions
plt.figure()
tp.annotate(f[f['frame'] == 0], frames[0]);

#track

search_range = 10 #see TrackPy documentation
t = tp.link(f, search_range, memory=0)

#filter out tracks where  nucleus is not present in all frames
t1 = tp.filter_stubs(t, len(frames))

# Compare the number of particles in the unfiltered and filtered data.
print('Before:', t['particle'].nunique())
print('After:', t1['particle'].nunique())

#re-indexing particles
old_particles = t1['particle'].values
new_particles = old_particles
for ind,part in enumerate(np.unique(old_particles)):
  indexes = np.where(old_particles == part)

  new_particles[indexes[0]] = ind

t1['particle'] = new_particles

#visualize tracks kept 
plt.figure()
tp.annotate(t1[t1['frame'] == 0], frames[0]);

#visualize track evolution

plt.figure(1)

for frame in np.arange(len(frames)):
  tp.plot_traj(t1,superimpose=frames[frame])
  plt.show()
  sleep(0.2)
  clear_output(wait=True)

#get roi coordinates

rois = read_roi_zip('PATH_TO_FOLDER/RoiSet.zip')
rois = pd.DataFrame(rois).transpose()

#create a column with roi names (for comparison w/ rois dataframe)
dfroi = t1.copy()

names = [] #saving roi name
x_bounds = [] #saving x coords from segmentation mask
y_bounds = [] #saving y coords from segmentation mask

#this next part is specific to method suggested for saving rois and roi info
for ind, row in dfroi.iterrows():
  all_name = row['Label']
  name_index = all_name.find('0')
  cut_name = all_name[name_index:name_index+14]
  names.append(cut_name) #find image name 
  
  #find matching rows in rois df and general df
  match_rois = rois.loc[rois['name'] == cut_name]
  x_bounds.append(match_rois['x'].values)
  y_bounds.append(match_rois['y'].values)


dfroi['roi name'] = names
dfroi['x bounds'] = x_bounds
dfroi['y bounds'] = y_bounds

dfroi = dfroi.drop(columns = ['Label'])
dfroi.index = np.arange(dfroi.shape[0])


#obtain the patches and input into new dataframe

df_patches = pd.DataFrame() #intensity patches
df_bb = pd.DataFrame() #bounding boxes
df_contours = pd.DataFrame() #contour-only

no_nuclei = np.unique(dfroi['particle'])

patches_dfroi = list(np.zeros(dfroi.shape[0])) #add patches to main df

for nmb in no_nuclei:
  rois_temp = dfroi.loc[dfroi['particle'] == nmb] #work with one particle at a time
  rois_temp = rois_temp.sort_values(by = 'frame')
  index_dfroi = rois_temp.index
  rois_temp.index = np.arange(rois_temp.shape[0])

  patches = []
  bbs = []
  contours = []
  for ind, row in rois_temp.iterrows(): #one timeframe at a time
    time = row['frame']
    topx = int(np.floor(row['bx']))
    if topx < 0:
      topx = 0
    topy = int(np.floor(row['by']))
    if topy < 0:
      topy = 0
    bottomx = int(np.ceil(row['bx'] + row['w'])) #bb coordinates point to top left corner if bounding box. add height and width fo obtain total coordinates
    bottomy = int(np.ceil(row['by'] + row['h']))
    img = frames[ind] #opens image

    x_coords = row['x bounds'][0]
    y_coords = row['y bounds'][0]
    coords = np.array(np.transpose(np.vstack((x_coords,y_coords))))
    coords = np.rint(coords).astype(np.int32)
    mask = np.zeros((img.shape[0], img.shape[1])) #create one mask of entire image for each roi

    cv2.fillConvexPoly(mask, coords, 1)
    mask = mask.astype(np.bool) #mask is binary

    out = np.zeros_like(img)
    out[mask] = img[mask] #image from mask, but keeping original intensities where mask== true

    patch = np.array(out[topy:bottomy,topx:bottomx]) # patch is a bb created from the mask that keeps original intensities

    #creates contour
    ret,patchbin = cv2.threshold(patch,0,255,cv2.THRESH_BINARY)
    # Find largest contour in intermediate image
    cnts, hierarqy = cv2.findContours(patchbin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
    cnt = max(cnts, key=cv2.contourArea)
    # Output
    cntr = np.zeros(patch.shape, np.uint8)
    cv2.drawContours(cntr, cnt, -1, 255, cv2.FILLED)

    patches_dfroi[index_dfroi[ind]] = patch #saves patch in main df

    patches.append(patch) #saves all patches
    contours.append(cntr) #saves all contours
    bbs.append([topy,topx,bottomy-topy,bottomx-topx]) #saves all bb coordinates


  df_patches[nmb] = patches
  df_bb[nmb] = bbs
  df_contours[nmb] = contours

#add patches to main df
dfroi['Patch'] = patches_dfroi

df_patches = df_patches.transpose()
df_patches.index = np.arange(df_patches.shape[0])
df_bb = df_bb.transpose()
df_bb.index = np.arange(df_patches.shape[0])
df_contours = df_contours.transpose()
df_contours.index = np.arange(df_contours.shape[0])


#save dataframte

df_patches.to_pickle('OUTPUT_PATH/patches.pickle')
df_bb.to_pickle('OUTPUT_PATH/bbs.pickle')
df_contours.to_pickle('OUTPUT_PATH/contours.pickle')
dfroi.to_pickle('OUTPUT_PATH/dfroi.pickle')

#print all the nuclei and all the frames


for indx in np.arange(df_patches.shape[0]):
  df = df_patches.loc[indx]
  df_c = df_contours.loc[indx]
  
  fig, axes = plt.subplots(nrows=3, ncols=7, figsize=(15,10))
  fig.suptitle('Nucleus ' + str(indx))
  fig2, axes2 = plt.subplots(nrows=3, ncols=7, figsize=(15,10))
  fig2.suptitle('Nucleus ' + str(indx))

  
  for ind,row in enumerate(df):
    axes.flatten()[ind].set_title( 'Frame ' + str(ind)) 
    axes.flatten()[ind].imshow(row)
    axes2.flatten()[ind].set_title( 'Frame ' + str(ind)) 
    axes2.flatten()[ind].imshow(df_c[ind])

  plt.show()


"""# ***Calculating Trajectory features***"""


#read the dataframe

df = pd.read_pickle('OUTPUT_PATH/dfroi.pickle')


#calculating features

particles = np.unique(df['particle'].values)
timeframes = np.unique(df['frame'])


#save features
orientation_t = []
norm_t = []
norm_tx = []
norm_ty = []



distances = pd.DataFrame(index = timeframes, columns=particles) #saves distances at each timeframe
distancesx = pd.DataFrame(index = timeframes, columns=particles) #saves distances for axis x at each timeframe
distancesy = pd.DataFrame(index = timeframes, columns=particles) #saves distances for axis y at each timeframe
orientations = pd.DataFrame(index = timeframes, columns=particles) #saves orientation of step at each timeframe
coordsx = pd.DataFrame(index = timeframes, columns=particles) #saves coordinates in x axis 
coordsy = pd.DataFrame(index = timeframes, columns=particles) #saves coordinates in y axis

df_traj = pd.DataFrame(index = particles) #saves all features for trajectory, for each nucleus


for indx in particles:

  df_part = df.loc[df['particle'] == indx] #go particle by particle
  df_part = df_part.sort_values(by = ['frame'])
  df_part.index = np.arange(df_part.shape[0])

  #mostly calculating net values
  centers = df_part['centroid (um)'].values
  init = centers[0]
  end = centers[-1]
  norm = np.linalg.norm(end-init)
  norm_t.append(norm)
  norm_tx.append(end[0]-init[0])
  norm_ty.append(end[1]-init[1])
  vector = np.array(end) - np.array(init)
  angle_g = np.arctan2(-vector[0],vector[1])
  orientation_t.append(angle_g)

  #going frame-by-frame
  for idx, row in df_part.iterrows():
    if idx == 0:
      prev_c = np.asarray(row['centroid (um)'])
      prev_cx = np.asarray(row['x (um)'])
      prev_cy = np.asarray(row['y (um)'])

    norm_p = np.linalg.norm(np.array(row['centroid (um)']) - prev_c)
    normx_p = np.linalg.norm(np.array(row['x (um)']) - prev_cx)
    normy_p = np.linalg.norm(np.array(row['y (um)']) - prev_cy)
    
    vector = np.array(row['centroid (um)']) - prev_c
    angle = np.arctan2(-vector[0],vector[1])
    
    prev_c = np.array(row['centroid (um)'])
    prev_cx = np.array(row['x (um)'])
    prev_cy = np.array(row['y (um)'])

    coordsx.iat[idx,indx] = row['x (um)']
    coordsy.iat[idx,indx] = row['y (um)']
    distances.iat[idx,indx] = norm_p
    distancesx.iat[idx,indx] = normx_p
    distancesy.iat[idx,indx] = normy_p 

    orientations.iat[idx,indx] = angle


#correcting for limitations of the angle calculation function
for columnName, columnData in orientations.iteritems():

  unwraped = np.unwrap(columnData, discont = 6)
  orientations[columnName] = unwraped


df_traj['Net Orientation'] = orientation_t #angle between initial and final positions
df_traj['Net Displacement'] = norm_t #total distance (euclidean)
df_traj['Net Displacement X'] = norm_tx #total distance, x axis
df_traj['Net Displacement Y'] = norm_ty #total distance, y axis

#get total distances

total_dist = []
total_distx = []
total_disty = []
mean_step = []
std_step = []

for columnName, columnData in distances.iteritems():

  dist = np.sum(columnData)
  mean_step.append(np.mean(columnData))
  std_step.append(np.std(columnData))

  total_dist.append(dist)

for columnName, columnData in distancesx.iteritems():

  dist = np.sum(columnData)
  total_distx.append(dist)

for columnName, columnData in distancesy.iteritems():

  dist = np.sum(columnData)
  total_disty.append(dist)

df_traj['Mean Step Displacement'] = mean_step #mean step distance in T
df_traj['Std Step Displacement'] = std_step #std deviation of step distance in T
df_traj['Total Distance'] = total_dist #total euclidean distance across time frames
df_traj['Total Distance X'] = total_distx #total x distance across time frames
df_traj['Total Distance Y'] = total_disty #total y distance across time frames
df_traj['Efficiency'] = df_traj['Net Displacement'].values / np.array(total_dist) #migration efficiency: net distance over total distance

#now, some orientation features

mean_ang = []
std_ang = []
tot_ang = []
avg_straight = []
avg_bend = []
mean_abs_ang = []
std_abs_ang = []
mean_dir = []

for columnName, columnData in orientations.iteritems():
  
  mean_ang.append(np.mean(columnData))
  mean_abs_ang.append(np.mean(np.abs(columnData)))
  std_ang.append(np.std(columnData))
  std_abs_ang.append(np.std(np.abs(columnData)))
  tot_ang.append(np.sum(np.abs(columnData)))
  avg_straight.append(np.sum(np.cos(columnData)) / columnData.shape[0])
  avg_bend.append(np.sum(np.sin(columnData)) / columnData.shape[0])
  dir = []
  for ind, row in enumerate(columnData):
    if ind == 0:
      prev = row
    else:
      cur = row
      dir.append(np.abs(cur - prev))
  mean_dir.append(np.mean(dir))

df_traj['Mean Step Orientation'] = mean_ang #mean angle between steps
df_traj['Std Step Orientation'] = std_ang #std deviation of distribution of angle between steps
df_traj['Mean Abs Step Orientation'] = mean_abs_ang #mean angle between steps (in absolute value)
df_traj['Std Abs Step Orientation'] = std_abs_ang #std deviation of distribution of angle between steps (in absolute value)
df_traj['Total Step Orientation'] = tot_ang #total orientation variation (sum of all angles between steps in abs value)
df_traj['Average Straightness'] = avg_straight #straightness - defined as the mean cosine of the angle between steps
df_traj['Average Bending'] = avg_bend #bending - defined as the mean sine of the angle between steps
df_traj['Mean Direction Variation'] = mean_dir #mean direction


#Calculate MSD (um)

MSDs = pd.DataFrame(index = timeframes, columns=particles) #saves msd for each particle, at each tf

MSDs.loc[0] = np.zeros(MSDs.shape[1])
for columnName in coordsx.columns:
  xs = coordsx[columnName]
  ys = coordsy[columnName]

  for lag in np.arange(1, xs.shape[0]):
    msd_lag = []
    for ind, row in enumerate(xs):
      x_prev = row
      y_prev = ys[ind]
      
      if (ind + lag) < xs.shape[0]:
        x = xs[ind + lag]
        y = ys[ind + lag]
        msd_temp =  np.power((x-x_prev),2) + np.power((y-y_prev),2) #calculates MSD for that lag
        msd_lag.append(msd_temp)

      else:
        break
      
    avg_lag = np.mean(np.array(msd_lag))

    MSDs.iat[lag,columnName] = avg_lag



#calculate powerlaw

# Function to calculate the power-law with constants a and b
def power_law(x, D, beta):
  """
  input for f, in curve_fit function (from scipy)
  ...
  Inputs for curve_fit
  f — function used for fitting (in this case exponential)
  xdata — array of x-data for fitting
  ydata — array of y-data for fitting
  p0 — array of initial guesses for the fitting parameters (both a and b as 0)
  bounds — bounds for the parameters (-∞ to ∞)
  Outputs
  pars — array of parameters from fit (in this case [a, b])
  cov — the estimated covariance of pars which can be used to determine the standard deviations of the fitting parameters (square roots of the diagonals)
  """
  return D*np.power(x, beta)

time_interval = 3 #TIME IN MINUTES, MAY NEED TO CHANGE


x = MSDs.index[1:]*time_interval


Ds = []
betas = []
stdevs = []
resds = []
rs = []

#for plotting
fig, ax = plt.subplots(figsize = (15,15))

# Set the x and y-axis scaling to logarithmic
ax.set_xscale('log')
ax.set_yscale('log')

# Edit the major and minor tick locations of x and y axes
ax.xaxis.set_major_locator(mpl.ticker.LogLocator(base=10.0))
ax.yaxis.set_major_locator(mpl.ticker.LogLocator(base=10.0))



for columnName,columnData in MSDs.iteritems():

  columnData = np.array(columnData).astype(float)[1:]


  ax.scatter(x,columnData, s = 3, alpha = 0.5) #plot
  

  pars, cov = curve_fit(f = power_law, xdata = x, ydata = columnData, method = 'lm') #fits data to a power law
  Ds.append(pars[0]) #saves D
  betas.append(pars[1]) #saves exponentn index
  ax.plot(x,power_law(x,pars[0],pars[1]), alpha = 0.5)

  # Get the standard deviations of the parameters (square roots of the # diagonal of the covariance)
  stdev = np.sqrt(np.diag(cov))
  stdevs.append(stdev)
  # Calculate the residuals
  res = columnData - power_law(x, *pars)
  resds.append(res)
  correlation_matrix = np.corrcoef(columnData,power_law(x,pars[0],pars[1]))
  correlation_xy = correlation_matrix[0,1]
  r_squared = correlation_xy**2
  rs.append(r_squared)
  

df_traj['D'] = Ds
df_traj['beta'] = betas
df_traj['Std (powerlaw)'] = stdevs
df_traj['Residuals (powerlaw)'] = resds
df_traj['R2'] = rs


#plots MSDs

def point_plot(MSDs, time_interval, histogram = True):
  """
  Plots MSDs, calculates avg MSD
  """

  from seaborn import pointplot as pointplot
  import seaborn as sns
  from matplotlib.colors import LinearSegmentedColormap
  import matplotlib.patches as mpatches
  import matplotlib as mpl
  mpl.rcParams.update({'font.size': 22})
  

  #plotting parameters
  fig, ax = plt.subplots(figsize=(17,10), dpi = 300)
  mpl.rcParams['lines.markersize'] = 10
  ax.set(xscale="log", yscale = "log")
  ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)



  for columnName,columnData in MSDs.iteritems():
      
      msds = columnData
      msds.index = np.arange(msds.shape[0])*time_interval
      ax.plot(msds.index[1:], msds.values[1:], linewidth = 0.75,markersize = 10, marker = 'o', mec='k',zorder = 0, alpha = 0.4)

  #more plot parameters
  ax.set_xticks([3,6,9,12,15,18,21,24,27,30,45,60])
  ax.get_xaxis().set_major_formatter(mpl.ticker.ScalarFormatter())
  ax.get_yaxis().set_major_formatter(mpl.ticker.ScalarFormatter())

  ax.set_title('MSD')
  ax.set_ylabel('MSD (\u03BC'+'m)\u00b2')
  ax.set_xlabel('Lag time (min)')
  
  ax.set_axisbelow(True)
  ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)


  #average MSD
  avg_msd = MSDs.mean(axis = 1)
  avg_msd.index = np.arange(avg_msd.shape[0])*time_interval
  ax.plot(avg_msd.index[1:], avg_msd[1:], linewidth = 0.75, c = 'k', markersize = 15, marker = 'D', mec='k',zorder = 1)
  
  plt.show()

  return avg_msd



#ADJUST TIME INTERVAL
time_interval = 3 #min

avg_msd = point_plot(MSDs, time_interval, True)


#save dfs
df_traj.to_pickle('OUTPUT_PATH/all_traj.pickle')
MSDs.to_pickle('OUTPUT_PATH/MSDs.pickle')

"""# ***Calculating Dynamic features*** """


#read the pandas dataframe

df_all = pd.read_pickle('OUTPUT_PATH/dfroi.pickle')

#calculate features

df_base = df_all['Patch'] 

df = pd.DataFrame(data = df_all.values, columns = df_all.columns)

final_morph = [] #morphological features


for index, row in enumerate(df_base):

        patch = row

        #calculate features
        propsmorph = RegionPropsMorph(patch)
        
        #obtain feature labels (invariant) and their values
        morph_labels, morph_values = propsmorph.print_features(print_values = False) #change to true to see feature values

        #add each feature to its respective list, which will then be put into the dataframe
        for i in np.arange(len(morph_values)):
          if len(final_morph) < len(morph_values):
                  final_morph.append([morph_values[i]])
          else:
              final_morph[i].append(morph_values[i])

      
#add features to dataframe and visualize

for i in np.arange(len(morph_labels)):
    df[morph_labels[i]] = final_morph[i]



#save the new dataframe

df.to_pickle('OUTPUT_PATH/dfroi.pickle')

#if wanting to save to xlsx file, uncomment the following:
#excel_name = 'all.xlsx'
#df_path =  excel_name
#df_all.to_excel(df_path)

"""# ***Dynamic Feature Analysis*** """


#read the pandas dataframe

df = pd.read_pickle('OUTPUT_PATH/dfroi.pickle')


#select columns to calculate relative values of

features = ['Area','Perimeter','Equivalent Diameter', 'Major Axis Length',
       'Minor Axis Length', 'Eccentricity', 'Circularity', 'Roundness',
       'Aspect Ratio', 'Orientation', 'Solidity', 'Roughness']

#calculate relative values, as percentage, absolute percentage and net change

df_variation = pd.DataFrame() #relative variation as %
df_abs_variation = pd.DataFrame()  #absolute relative variation as %

df_net_variation = pd.DataFrame() #relative net variation of features as %



for feat in features: #go feature by feature

  df_feats = df[[feat, 'particle', 'frame']]
  net_vars = []
  relative_list = []
  relative_abs_list = []

  particle_no = [] #saves particle
  frames = [] #saves frames
                
  for particle in np.unique(df_feats['particle'].values): #go particle by particle

    df_feat = df_feats.loc[df_feats['particle'] == particle]

    df_feat = df_feat.sort_values(by = 'frame')

    
    

    particle_no.extend([particle]*(df_feat.shape[0] - 1))
    frames.extend(df_feat['frame'].values[1:])

  
    for ind, value in enumerate(np.array(df_feat[feat])): #go frame by frame

      if ind == 0:
        prev_feat = value

      else:
        new_feat = value
        delta = ((new_feat - prev_feat) / prev_feat)*100   #given as a percentage of the value from the previous frame
        delta_abs = np.abs(delta) #absolute relative variation

        relative_list.append(delta)
        relative_abs_list.append(delta_abs)

    

    first = df_feat.loc[df_feat['frame'] == 0][feat].values[0]
    last = df_feat.loc[df_feat['frame'] == np.max(df_feat['frame'])][feat].values[0]
    net_var = ((first-last/first))*100 #net variation, as a % of the value for the initial frame

    net_vars.append(net_var)

  df_variation[feat] = relative_list
  df_abs_variation[feat] = relative_abs_list

  df_net_variation[feat] = net_vars

df_variation['particle'] = particle_no
df_abs_variation['particle'] = particle_no
df_net_variation['particle'] = np.unique(np.array(particle_no))

df_variation['frame'] = frames
df_abs_variation['frame'] = frames
